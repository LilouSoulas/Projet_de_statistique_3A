{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60da8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features.csv\", sep=\";\")\n",
    "df = df.drop('semanticobjscore', axis=1) \n",
    "df = df.drop('semanticsubjscore', axis=1) \n",
    "df = df.drop('URL', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beca2aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['objective' 'subjective']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.drop(['Label', 'TextID'], axis=1)  \n",
    "y = df['Label']\n",
    "\n",
    "# Convertir les labels en nombres\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(\"Classes:\", le.classes_)  # Pour voir la correspondance\n",
    "\n",
    "# Refaire la séparation train/test/val avec les labels encodés\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=3,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.25,\n",
    "    random_state=3,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ecefd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle...\n",
      "Iteration 1, loss = 7.94522992\n",
      "Iteration 2, loss = 5.71048769\n",
      "Iteration 3, loss = 3.63920185\n",
      "Iteration 4, loss = 2.50007357\n",
      "Iteration 5, loss = 2.53778353\n",
      "Iteration 6, loss = 1.81859237\n",
      "Iteration 7, loss = 1.41921459\n",
      "Iteration 8, loss = 1.26513283\n",
      "Iteration 9, loss = 1.13438602\n",
      "Iteration 10, loss = 0.96698646\n",
      "Iteration 11, loss = 0.84822269\n",
      "Iteration 12, loss = 0.86998834\n",
      "Iteration 13, loss = 0.73834944\n",
      "Iteration 14, loss = 0.74539767\n",
      "Iteration 15, loss = 0.72717248\n",
      "Iteration 16, loss = 0.69485968\n",
      "Iteration 17, loss = 0.67831169\n",
      "Iteration 18, loss = 0.67863872\n",
      "Iteration 19, loss = 0.64883305\n",
      "Iteration 20, loss = 0.62376573\n",
      "Iteration 21, loss = 0.62164284\n",
      "Iteration 22, loss = 0.60876087\n",
      "Iteration 23, loss = 0.57811315\n",
      "Iteration 24, loss = 0.56658762\n",
      "Iteration 25, loss = 0.55256591\n",
      "Iteration 26, loss = 0.53608111\n",
      "Iteration 27, loss = 0.51722324\n",
      "Iteration 28, loss = 0.51216874\n",
      "Iteration 29, loss = 0.50623162\n",
      "Iteration 30, loss = 0.51958057\n",
      "Iteration 31, loss = 0.56243450\n",
      "Iteration 32, loss = 0.54004045\n",
      "Iteration 33, loss = 0.56899022\n",
      "Iteration 34, loss = 0.59783333\n",
      "Iteration 35, loss = 0.57903374\n",
      "Iteration 36, loss = 0.62313086\n",
      "Iteration 37, loss = 0.62544657\n",
      "Iteration 38, loss = 0.68173997\n",
      "Iteration 39, loss = 0.59891661\n",
      "Iteration 40, loss = 0.53170310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Entraînement terminé!\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS DU NEURAL NETWORK\n",
      "==================================================\n",
      "Train Accuracy: 0.8517\n",
      "Validation Accuracy: 0.8300\n",
      "Train Precision: 0.8186\n",
      "Validation Precision: 0.8000\n",
      "Train Recall: 0.7626\n",
      "Validation Recall: 0.7123\n",
      "Train F1: 0.7896\n",
      "Validation F1-score: 0.7536\n",
      "\n",
      "\n",
      "\n",
      "==================================================\n",
      "INFOS SUR LE MODÈLE\n",
      "==================================================\n",
      "Nombre de couches : 3\n",
      "Nombre de paramètres (poids) : 4\n",
      "Iterations effectuées : 40\n",
      "Loss (erreur finale) : 0.5317\n"
     ]
    }
   ],
   "source": [
    "# NEURAL NETWORK STEP BY STEP\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val) \n",
    "X_test_scaled = scaler.transform\n",
    "\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  \n",
    "    max_iter=1000,                  \n",
    "    random_state=3,\n",
    "    verbose=1                       \n",
    ")\n",
    "\n",
    "# ===== ÉTAPE 3 : Entraîner le modèle =====\n",
    "# Utilise les données d'entraînement normalisées\n",
    "print(\"Entraînement du modèle...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Entraînement terminé!\\n\")\n",
    "\n",
    "# ===== ÉTAPE 4 : Faire des prédictions =====\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val = model.predict(X_val)\n",
    "\n",
    "# ===== ÉTAPE 5 : Évaluer les performances =====\n",
    "print(\"=\" * 50)\n",
    "print(\"RÉSULTATS DU NEURAL NETWORK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "val_acc = accuracy_score(y_val, y_pred_val) \n",
    "val_precision = precision_score(y_val, y_pred_val)\n",
    "val_recall = recall_score(y_val, y_pred_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "train_precision = precision_score(y_train, y_pred_train)\n",
    "train_recall = recall_score(y_train, y_pred_train)\n",
    "train_f1 = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Train Precision: {train_precision:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Train Recall: {train_recall:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Train F1: {train_f1:.4f}\")\n",
    "print(f\"Validation F1-score: {val_f1:.4f}\")\n",
    "print(\"\\n\")\n",
    "# ===== ÉTAPE 6 (OPTIONNEL) : Infos sur le modèle =====\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"INFOS SUR LE MODÈLE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nombre de couches : {len(model.hidden_layer_sizes) + 1}\")  # +1 pour output\n",
    "print(f\"Nombre de paramètres (poids) : {model.n_layers_}\")\n",
    "print(f\"Iterations effectuées : {model.n_iter_}\")\n",
    "print(f\"Loss (erreur finale) : {model.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07fe20",
   "metadata": {},
   "source": [
    "La fonction MLPClassifier prend par défaut ReLu comme fonction d'activation et la fonction sigmoïde pour la couche de sortie (car on est dans un cas binaire). L'algorithme par défaut est l'algorithme Adam (avec taux d'apprentissage 0,001 et taux de régularisation 0,0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc0fdf",
   "metadata": {},
   "source": [
    "LÀ ON VA UTILISER RANDOMSEARCH POUR CHOISIR LES HYPERPARAMÈTRES AU MIEUX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import loguniform, uniform, randint # Pour définir des distributions\n",
    "from sklearn.metrics import f1_score # La métrique sur laquelle arbitrer\n",
    "\n",
    "# Définition du modèle de base (sans hyperparamètres spécifiques)\n",
    "# C'est l'objet qui sera cloné et entraîné par la recherche aléatoire\n",
    "model_base = MLPClassifier(max_iter=2000, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Nous définissons l'espace de recherche (le \"param_dist\")\n",
    "param_dist = {\n",
    "    # 1. Architecture (hidden_layer_sizes) : C'est une liste de tuples à choisir\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50), (128, 64)], \n",
    "    \n",
    "    # 2. Régularisation (alpha) : Loguniform est idéal pour les paramètres de régularisation\n",
    "    'alpha': loguniform(1e-5, 1e-2), # Exemple: choisir une valeur entre 0.00001 et 0.01\n",
    "    \n",
    "    # 3. Optimisation (learning_rate_init) : Loguniform est aussi bon pour le taux d'apprentissage\n",
    "    'learning_rate_init': loguniform(1e-4, 1e-2), # Exemple: choisir une valeur entre 0.0001 et 0.01\n",
    "    \n",
    "    # 4. Fonction d'activation : Une liste de choix discrets\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    \n",
    "    # 5. Type de solveur : Une liste de choix discrets\n",
    "    'solver': ['adam', 'sgd'] \n",
    "}\n",
    "\n",
    "\n",
    "# Instanciation de RandomizedSearchCV\n",
    "# n_iter=50 : Le nombre total d'itérations aléatoires que nous voulons tester (compromis temps/précision)\n",
    "# scoring='f1' : La métrique sur laquelle nous arbitrons (maximiser le F1-score)\n",
    "# cv=5 : Utilise la validation croisée K-Fold avec 5 plis (plus fiable qu'un simple split Train/Val)\n",
    "# verbose=2 : Affiche le progrès détaillé\n",
    "# n_jobs=-1 : Utilise tous les cœurs du processeur pour accélérer le calcul\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_base,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50, \n",
    "    scoring='f1', \n",
    "    cv=5, \n",
    "    random_state=42, \n",
    "    verbose=2,\n",
    "    n_jobs=-1 \n",
    ")\n",
    "\n",
    "# X_train et y_train sont les données utilisées pour la recherche d'hyperparamètres.\n",
    "# L'objet random_search effectue l'entraînement et la validation croisée en interne.\n",
    "print(\"Début de la recherche aléatoire...\")\n",
    "random_search.fit(X_train, y_train) \n",
    "print(\"Recherche aléatoire terminée.\")\n",
    "\n",
    "\n",
    "# Récupération du meilleur jeu d'hyperparamètres (celui qui a maximisé le F1-score CV)\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Récupération du modèle qui a obtenu le meilleur score de validation croisée\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"\\n---------------------------------------------\")\n",
    "print(\"MEILLEURS HYPERPARAMÈTRES TROUVÉS (JUSTIFICATION) :\")\n",
    "print(best_params)\n",
    "print(f\"Meilleur score de validation croisée (F1-score) : {random_search.best_score_:.4f}\")\n",
    "print(\"---------------------------------------------\")\n",
    "\n",
    "# Évaluation finale sur l'ensemble de TEST\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Performance finale sur la base de TEST : F1-score = {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
