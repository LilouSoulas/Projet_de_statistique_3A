
Pour la séparation des bases : 

Il faut séparer en 
- Entraînement (60-80% de la base) sur lequel le modèle s'entraîne
- Base de validation (10-20% de la base) pour ajuster / modifier les hyperparamètres sans toucher encore à la base test 
- Test (10-20%) sert du test à la toute fin lorsque les hyperparamètres on été ajustés, c'est selon les résultats du modèle sur cette base qu'on évalue le modèle 

Comment séparer concrètement ? 
- from sklearn.model_selection import train_test_split

# Première séparation : 80% entraînement/validation, 20% test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Deuxième séparation : diviser les 80% en 75% entraînement, 25% validation
# (ce qui donne 60% entraînement, 20% validation, 20% test au final)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

Utiliser stratify = colonne_label dans la fonction train_test_split sinon pb 



Les ≠ tests / métriques : 
Accuracy : sur tous les cas, quel % j'ai bien classé ?
Accuracy=VP+VN/VP+VN+FP+FN

Précision : parmi les cas que j'ai prédits positifs, combien étaient vraiment positifs ?

Précision=VP/VP+FP

Recall (sensibilité) : parmi les vrais positifs, combien j'en ai détecté ?

Recall=VP/VP+FN\

F1 : moyenne harmonique entre précision et recall (balance les deux)
F1=2×Précision×Recall/Précision+Recall


* Accuracy : général, données équilibrées
* Précision : quand les faux positifs coûtent cher (spam detection)
* Recall : quand les faux négatifs coûtent cher (détection cancer)
* F1 : quand tu veux un équilibre entre les deux



